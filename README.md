# Multi-Threaded Quotes Scraper ğŸ“ğŸš€

## ğŸ“– Project Overview

This Python script is designed to:
- Connect to a MongoDB database.
- Retrieve category URLs with a status of `pending`.
- Scrape quotes, authors, and tags from each paginated category page.
- Store the extracted data in a MongoDB collection.
- Export the collected data to CSV and Excel formats.
- Use multi-threading for faster scraping.

---

## ğŸ“‚ Key Features

âœ… Multi-threaded scraping using `ThreadPoolExecutor`  
âœ… Pagination handling until no next page or request limit is reached  
âœ… Automatically marks category status as 'done' after processing  
âœ… Exports data to both CSV and Excel formats  

---

## ğŸ› ï¸ Technologies Used
- `requests` â€” for HTTP requests  
- `lxml` â€” for HTML parsing using XPath  
- `pymongo` â€” to interact with MongoDB  
- `concurrent.futures` â€” for multi-threading  
- `pandas` â€” for data export to CSV and Excel  

---

## ğŸ“¦ MongoDB Collections
| Collection Name   | Purpose                                                      |
|-------------------|--------------------------------------------------------------|
| `category_urls`   | Stores category URLs with their status (`pending` or `done`) |
| `quotes_data`     | Stores scraped quotes, authors, and tags                    |

---

## âš™ï¸ User Inputs (Prompted at Runtime)
| Input                | Description                                               |
|----------------------|-----------------------------------------------------------|
| `request_limit`      | The maximum number of HTTP requests the script will send |
| `max_workers`        | The number of concurrent threads for scraping            |

---

## ğŸ“œ How to Run

### 1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/your-username/multi-threaded-quotes-scraper.git

cd multi-threaded-quotes-scraper
